#!/bin/bash
#SBATCH --nodes=32
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --time=18:00:00
#SBATCH --mem=10GB
#SBATCH --job-name=anneal_size
#SBATCH --mail-type=END
#SBATCH --mail-user=jg6848@nyu.edu
#SBATCH --output=slurm_anneal_%j.out

module purge
module load openmpi/gcc/4.0.5

# vary problem size
mpiexec -n 32 ./SA_main -x "100.txt" -j 500 -i 100000 -v "viz/size/size100_proc32.txt" -t 5 > "results/size/exp32_100.txt"
mpiexec -n 32 ./SA_main -x "200.txt" -j 500 -i 100000 -v "viz/size/size200_proc32.txt" -t 5 > "results/size/exp32_200.txt"
mpiexec -n 32 ./SA_main -x "400.txt" -j 500 -i 100000 -v "viz/size/size400_proc32.txt" -t 5 > "results/size/exp32_400.txt"
mpiexec -n 32 ./SA_main -x "800.txt" -j 500 -i 100000 -v "viz/size/size800_proc32.txt" -t 5 > "results/size/exp32_800.txt"
mpiexec -n 32 ./SA_main -x "1600.txt" -j 500 -i 100000 -v "viz/size/size1600_proc32.txt" -t 5 > "results/size/exp32_1600.txt"
mpiexec -n 32 ./SA_main -x "3200.txt" -j 500 -i 100000 -v "viz/size/size3200_proc32.txt" -t 5 > "results/size/exp32_3200.txt"

# vary number of nodes keeping problem sized fixed 
mpiexec -n 1 ./SA_main -x "400.txt" -j 500 -i 1000000 -v "viz/machines/size400_proc1.txt" -t 5 > "results/machines/exp1_400.txt"
mpiexec -n 4 ./SA_main -x "400.txt" -j 500 -i 1000000 -v "viz/machines/size400_proc4.txt" -t 5 > "results/machines/exp4_400.txt"
mpiexec -n 8 ./SA_main -x "400.txt" -j 500 -i 1000000 -v "viz/machines/size400_proc8.txt" -t 5 > "results/machines/exp8_400.txt"
mpiexec -n 16 ./SA_main -x "400.txt" -j 500 -i 1000000 -v "viz/machines/size400_proc16.txt" -t 5 > "results/machines/exp16_400.txt"
mpiexec -n 32 ./SA_main -x "400.txt" -j 500 -i 1000000 -v "viz/machines/size400_proc32.txt" -t 5 > "results/machines/exp32_400.txt"

# run one big example
mpiexec -n 32 ./SA_main -x "100000.txt" -j 1000 -i 100000 -v "viz/size/size100k_proc32.txt" -t 3 > "results/size/exp32_100k.txt"
